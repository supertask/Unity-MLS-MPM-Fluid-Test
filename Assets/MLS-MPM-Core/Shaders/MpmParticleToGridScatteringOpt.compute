//
// 1. P2GScatteringOptでmass, mass_x_velocityを計算して_GridAndMassIdsBufferに保存
// 2. grid cell indexをキーに1. の配列をソート
// 3. BoundaryAndIntervalでBoundaryとIntervalを計算
// 4. GatherAndWriteでParallel reduction sumで計算してグリッドに書き出す
//
#pragma kernel ClearBuffers
#pragma kernel P2GScatteringOpt
#pragma kernel BoundaryAndInterval
#pragma kernel GatherAndWrite

//Assets/MLS-MPM-Core/Shaders
#include "MpmStruct.hlsl"
#include "./Constant.hlsl"
#include "./Grid.hlsl"
#include "./SVD.hlsl"

#define THREAD_1D 1024
#define THREAD_GATHER_1D 512
//#define THREAD_1D 256
//#define THREAD_1D 64

float _DeltaTime;
uint _NumOfParticles;


// lock-free based GPU optimazation
// x = grid index, y = lane index(particle num * neighbour grid num). Sorted by grid index
// Used in P2GScatteringOpt
// Used in P2GScatteringOpt
RWStructuredBuffer<uint2> _GridAndMassIdsBuffer;
RWStructuredBuffer<uint2> _GridPingPongBuffer;
RWStructuredBuffer<P2GMass> _P2GMassBuffer;
StructuredBuffer<P2GMass> _SortedP2GMassBuffer;

// if _GridBufferRead'x is 0000011111122222334444,
// _GridIndicesBuffer will be
// Grid cell index             0 1  2  3  4
// Start index of grid buffer  0 5  11 16 18
// End index of grid buffer    4 10 15 17 21
// Then particle index is now trackable by using Start and End index
StructuredBuffer<uint2> _GridIndicesBuffer;
RWStructuredBuffer<uint2> _BoundaryAndIntervalBuffer;

//
// Particles to Grid
//
// Parameters of hyper elastic material.
float _HyperElasticHardening;
float _HyperElasticMu;
float _HyperElasticLambda;
StructuredBuffer<MpmParticle> _ParticlesBufferRead;
RWStructuredBuffer<MpmCell> _GridBuffer;
RWStructuredBuffer<LockMpmCell> _LockGridBuffer;

int _CellNeighbourLength; //27
RWStructuredBuffer<int3> _CellNeighbourBuffer; //27 array

[numthreads(THREAD_1D,1,1)]
void ClearBuffers(uint3 DTid : SV_DispatchThreadID)
{
	uint laneId = DTid.x;
	//_GridAndMassIdsBuffer[laneId] = uint2(0xFFFFFFFF, 0xFFFFFFFF);
	//_GridPingPongBuffer[laneId] = uint2(0xFFFFFFFF, 0xFFFFFFFF);
	_GridAndMassIdsBuffer[laneId] = uint2(-1, -1);
	_GridPingPongBuffer[laneId] = uint2(0, 0);
	P2GMass p2gMass;
	p2gMass.mass = 0;
	p2gMass.mass_x_velocity = float3(0,0,0);
	_P2GMassBuffer[laneId] = p2gMass;
	_BoundaryAndIntervalBuffer[laneId] = uint2(0,0);
}

[numthreads(THREAD_1D,1,1)]
void P2GScatteringOpt(uint3 DTid : SV_DispatchThreadID)
{
    const uint particleId = DTid.x;
	//if (_NumOfParticles > particleId) return;

	MpmParticle particle = _ParticlesBufferRead[particleId];
	if (particle.type == TYPE__INACTIVE) return;

	float3x3 F = particle.Fe;
	float3x3 R = 0;
	float3x3 U ;
	float3 d;
	float3x3 V;

	float volume = particle.volume;
	float3x3 Dinv = InvApicD();
	
	GetSVD3D(F, U, d, V);
	R = mul(U, transpose(V));

	float e = 1;
	float mu = _HyperElasticMu;
	float lambda = _HyperElasticLambda;

	float J = determinant(F);
	if(particle.type == TYPE__SNOW)
	{
		e = exp(_HyperElasticHardening * (1 - particle.Jp));
	}
	// else
	// if(particle.type == TYPE__FLUID)
	// {
	// 	mu = 0;
	// 	J = particle.Jp;
	// }

	float mup = mu * e;
	float lambdap = lambda * e;
	float3x3 PF = mul((2 * mup * (F - R)), transpose(F)) + lambdap * (J - 1) * J ;

	//if(particle.type == TYPE__FLUID)
	//{
	//	float s = particle.Jp -1;
	//	PF = float3x3(s,0,0,0,s,0,0,0,s) * mu * 10;
	//}
	float3x3 stress = -(_DeltaTime * volume) * mul(Dinv , PF);
	float3x3 apic = stress + particle.mass * particle.C;


	// J = clamp(J, 0.6f,20.0f);
	// float3x3 FinvT = transpose(inverse(F));
	// float3x3 PF = (2.0f * mup * (F - R)) + lambdap * (J - 1.0f) * J * FinvT;
	// float3x3 stress = 1.0f / J * mul(PF, transpose(F));

	int3 centerCellIndex3D = ParticlePositionToCellIndex3D(particle.position);

	//for (int gx = -1; gx <= 1; ++gx) {
	//	for (int gy = -1; gy <= 1; ++gy) {
	//		for(int gz = -1; gz <=1; ++gz) {


	for (int ni = 0; ni < _CellNeighbourLength; ni++) {
		int3 cellIndex3D = centerCellIndex3D + _CellNeighbourBuffer[ni];
		uint cellIndex1D = CellIndex3DTo1D(cellIndex3D);

		//For debug
		//_GridAndMassIdsBuffer[particleId + ni * _NumOfParticles] = uint2(DTid.x, 27);

		//TODO(Tasuku): 後でここのInGridのifを消す
		if (InGrid(cellIndex3D))
		{
			float3 gridPositionWS = CellIndex3DToPositionWS(cellIndex3D);
			float weight = GetWeight(particle.position, _CellNeighbourBuffer[ni]);

			P2GMass p2gMass;
			p2gMass.mass = weight * particle.mass;
			p2gMass.mass_x_velocity = weight * (particle.mass * particle.velocity
				+ mul(apic, (gridPositionWS - particle.position)));
				
			//Debug!!!
			//p2gMass.mass = particle.velocity;
			//p2gMass.mass_x_velocity = particle.velocity;

			uint laneId = particleId + ni * _NumOfParticles;

			// Used for Parallel reduction sum on mass/mass_x_velocity calculation
			// _P2GMassBuffer will be sorted by grid cell index with _GridBufferWrite
			_P2GMassBuffer[laneId] = p2gMass;

			// x = grid cell index, y = lane id of _P2GMassBuffer
			// _GridAndMassIdsBuffer will be sorted by grid cell index. 
			// Used for Parallel reduction sum on mass/mass_x_velocity calculation
			_GridAndMassIdsBuffer[laneId] = uint2(cellIndex1D, laneId);

			// For debug
			//_GridAndMassIdsBuffer[particleId + ni * _NumOfParticles] = uint2(DTid.x, ni * _NumOfParticles);

			//#if defined(LScattering) || defined(LFScattering)
			//	_GridAndMassIdsBuffer[particleId + ni * _NumOfParticles] = uint2(DTid.x, 55);
			//#else
			//	_GridAndMassIdsBuffer[particleId + ni * _NumOfParticles] = uint2(DTid.x, 88);
			//#endif
		}
	}

}


//
// Ming Gao, Xinlei Wang, Kui Wu, ..,
// GPU Optimization of Material Point Methods,
// ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia), 2018
// https://dl.acm.org/doi/10.1145/3272127.3275044
// http://pages.cs.wisc.edu/~sifakis/papers/GPU_MPM.pdf
//
[numthreads(THREAD_1D,1,1)]
void BoundaryAndInterval(
    // Unique id of entire thread
    // numthreads.x * groups.x	
	uint3 DTid : SV_DispatchThreadID,

    // Group ID
    // example:
    //   if numthreads is (4,1,1)
    //   0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3
    uint3 groupId : SV_GroupID,

    // Group index which is converted groupThreadId(3D) to 1D's number.
    // example:
    //   if numthreads is (4,1,1)
    //   0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3
	uint GI : SV_GroupIndex)
{
	const uint laneId = DTid.x;
	if (laneId-1 < 0) { return; }
	const uint cellId = _GridAndMassIdsBuffer[laneId].x;
	if (cellId == -1) { return; }
	const uint prevCellId = _GridAndMassIdsBuffer[laneId-1].x;
	if (prevCellId == -1) { return; }

	// 
	// Mark a boundary between different cells
	//
	// laneId == 0 does not have previous cell id. That's why
	// cellId != prevCellId: if cell ids are different, it is boundary point
	// GI == 0: boundary of group shared
	//
	uint boundary = (laneId == 0 || cellId != prevCellId || GI == 0) ? 1 : 0;
	//uint boundary = (laneId == 0 || cellId != prevCellId) ? 1 : 0;

	uint laneIdEnd = _GridIndicesBuffer[cellId].y;
	bool isEmptyIndices = laneIdEnd == -1;

	//
	// Mark region interval
	//
	// If the end of lane index is less than group shared boundary point,
	// the end is _GridIndicesBuffer[cellId].y.
	// else, the end is group shared boundary point
	//
	//uint regionInterval = laneIdEnd - 1 - laneId;
	laneIdEnd = (laneIdEnd < (groupId.x + 1) * THREAD_1D)
		? laneIdEnd
		: (groupId.x + 1) * THREAD_1D;
	uint regionInterval = laneIdEnd - 1 - laneId;

	_BoundaryAndIntervalBuffer[laneId] = isEmptyIndices
		? uint2(0,0)
		: uint2(boundary, regionInterval);
}


groupshared P2GMass blockP2gMass[THREAD_GATHER_1D * 2];
//groupshared float blockMass[128];
//groupshared float3 blockMassXVelocity[128];

//void GatherAndWrite(
//	uint3 Gid  : SV_GroupID,
//	uint3 DTid : SV_DispatchThreadID,
//	uint3 GTid : SV_GroupThreadID,
//	uint  GI : SV_GroupIndex)
//{
[numthreads(THREAD_GATHER_1D,1,1)]
void GatherAndWrite(uint3 DTid : SV_DispatchThreadID, uint GI : SV_GroupIndex)
{
	//
	// Bug memo:
	// GatherAndWriteでバグっているのは，ほぼ間違いない，
	// GridIndicesBufferあたりでバグっているか?, そもそも_BoundaryAndIntervalBufferで計算ミスしてる?
	// ま，でもbitonic sortで空の値で埋めているところは消した方がいいよなーとは思う．
	//
	// そのた
	// 表示されないパーティクルを表示させる. 
	// GatherAndWriteでの計算の不要な空パーティクルをはじく
	//
	const uint laneId = DTid.x;
	uint2 cellAndMassId = _GridAndMassIdsBuffer[laneId];
	uint cellId  = cellAndMassId.x;
	uint massIndex  = cellAndMassId.y;

	uint2 boundaryAndInterval = _BoundaryAndIntervalBuffer[laneId];
	uint boundary = boundaryAndInterval.x;
	uint regionInterval = boundaryAndInterval.y;

	// Store each particle info of 256 threads
	P2GMass p2gMass;
	p2gMass.mass = 0;
	p2gMass.mass_x_velocity = 0;
	if (massIndex >= 0) { p2gMass = _SortedP2GMassBuffer[massIndex]; }
	blockP2gMass[GI] = p2gMass;
	GroupMemoryBarrierWithGroupSync();
	for(uint stride = 1; stride < THREAD_GATHER_1D; stride <<= 1)
	{
		if (stride <= regionInterval) {
			// stride <= interval
			// only sum within the group(same grid index)
			blockP2gMass[GI].mass += blockP2gMass[GI+stride].mass;
			blockP2gMass[GI].mass_x_velocity += blockP2gMass[GI+stride].mass_x_velocity;
		}
		GroupMemoryBarrierWithGroupSync();
	}

	// Only the boundary node (Leader node) needs to write
	if (boundary && cellId >= 0)
	{
		//LockMpmCell cell = _LockGridBuffer[cellId];
		//cell.mass = (int) (blockP2gMass[GI].mass * FLOAT_TO_INT_DIGIT_1);
		//cell.mass_x_velocity = (int) (blockP2gMass[GI].mass_x_velocity * FLOAT_TO_INT_DIGIT_1);
		//_LockGridBuffer[cellId] = cell; //AtomicAdd is applied

		float mass = blockP2gMass[GI].mass;
		float3 mass_x_velocity = blockP2gMass[GI].mass_x_velocity;

		//int intMass = (int) (mass * FLOAT_TO_INT_DIGIT_1);
		////int3 intMassXVelocity = (int3) (blockP2gMass[GI].mass_x_velocity * FLOAT_TO_INT_DIGIT_1);
		//int intMassXVelocityX = (int) (mass_x_velocity.x * FLOAT_TO_INT_DIGIT_1);
		//int intMassXVelocityY = (int) (mass_x_velocity.y * FLOAT_TO_INT_DIGIT_1);
		//int intMassXVelocityZ = (int) (mass_x_velocity.z * FLOAT_TO_INT_DIGIT_1);

		int2 splitMass = ConvertFloatToInt2(mass);
		int2 splitMassXVelocityX = ConvertFloatToInt2(mass_x_velocity.x);
		int2 splitMassXVelocityY = ConvertFloatToInt2(mass_x_velocity.y);
		int2 splitMassXVelocityZ = ConvertFloatToInt2(mass_x_velocity.z);


		//InterlockedAdd(_LockGridBuffer[cellId].mass, intMass);
		InterlockedAdd(_LockGridBuffer[cellId].mass, splitMass.x);
		InterlockedAdd(_LockGridBuffer[cellId].mass2, splitMass.y);

		//InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity.x, intMassXVelocityX);
		//InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity.y, intMassXVelocityY);
		//InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity.z, intMassXVelocityZ);
		InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity.x, splitMassXVelocityX.x );
		InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity2.x, splitMassXVelocityX.y );
		InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity.y, splitMassXVelocityY.x );
		InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity2.y, splitMassXVelocityY.y );
		InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity.z, splitMassXVelocityZ.x );
		InterlockedAdd(_LockGridBuffer[cellId].mass_x_velocity2.z, splitMassXVelocityZ.y );
	}
}
