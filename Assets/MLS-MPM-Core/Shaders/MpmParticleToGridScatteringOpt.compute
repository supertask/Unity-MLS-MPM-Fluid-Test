//
// 1. P2GScatteringOptでmass, mass_x_velocityを計算して_GridAndMassIdsBufferに保存
// 2. grid cell indexをキーに1. の配列をソート
// 3. BoundaryAndIntervalでBoundaryとIntervalを計算
// 4. GatherAndWriteでParallel reduction sumで計算してグリッドに書き出す
//
#pragma kernel ClearBuffers
#pragma kernel P2GScatteringOpt
#pragma kernel BoundaryAndInterval
#pragma kernel GatherAndWrite

//Assets/MLS-MPM-Core/Shaders
#include "MpmStruct.hlsl"
#include "./Constant.hlsl"
#include "./Grid.hlsl"
#include "./SVD.hlsl"

//#define THREAD_1D 256
#define THREAD_1D 64

float _DeltaTime;
int _NumOfParticles;


// lock-free based GPU optimazation
// x = grid index, y = lane index(particle num * neighbour grid num). Sorted by grid index
// Used in P2GScatteringOpt
// Used in P2GScatteringOpt
RWStructuredBuffer<uint2> _GridAndMassIdsBuffer;
RWStructuredBuffer<uint2> _GridPingPongBuffer;
RWStructuredBuffer<P2GMass> _P2GMassBuffer;

// if _GridBufferRead'x is 0000011111122222334444,
// _GridIndicesBuffer will be
// Grid cell index             0 1  2  3  4
// Start index of grid buffer  0 5  11 16 18
// End index of grid buffer    4 10 15 17 21
// Then particle index is now trackable by using Start and End index
StructuredBuffer<uint2> _GridIndicesBuffer;
RWStructuredBuffer<uint2> _BoundaryAndIntervalBuffer;

//
// Particles to Grid
//
// Parameters of hyper elastic material.
float _HyperElasticHardening;
float _HyperElasticMu;
float _HyperElasticLambda;
StructuredBuffer<MpmParticle> _ParticlesBufferRead;
//RWStructuredBuffer<MpmCell> _GridBuffer;
RWStructuredBuffer<LockMpmCell> _GridBuffer;

int _CellNeighbourLength; //27
RWStructuredBuffer<int3> _CellNeighbourBuffer; //27 array

[numthreads(THREAD_1D,1,1)]
void ClearBuffers(uint3 DTid : SV_DispatchThreadID)
{
	uint laneId = DTid.x;
	//_GridAndMassIdsBuffer[laneId] = uint2(0xFFFFFFFF, 0xFFFFFFFF);
	//_GridPingPongBuffer[laneId] = uint2(0xFFFFFFFF, 0xFFFFFFFF);
	_GridAndMassIdsBuffer[laneId] = uint2(0, 0);
	_GridPingPongBuffer[laneId] = uint2(0, 0);
	P2GMass p2gMass;
	p2gMass.mass = 0;
	p2gMass.mass_x_velocity = 0;
	_P2GMassBuffer[laneId] = p2gMass;
	_BoundaryAndIntervalBuffer[laneId] = uint2(0,0);
}

[numthreads(THREAD_1D,1,1)]
void P2GScatteringOpt(uint3 DTid : SV_DispatchThreadID)
{
    const uint particleId = DTid.x;
	MpmParticle particle = _ParticlesBufferRead[particleId];
	if (particle.type == TYPE__INACTIVE) return;

	float3x3 F = particle.Fe;
	float3x3 R = 0;
	float3x3 U ;
	float3 d;
	float3x3 V;

	float volume = particle.volume;
	float3x3 Dinv = InvApicD();
	
	GetSVD3D(F, U, d, V);
	R = mul(U, transpose(V));

	float e = 1;
	float mu = _HyperElasticMu;
	float lambda = _HyperElasticLambda;

	float j = determinant(F);
	//if(particle.type == 2)
	//{
		e = exp(_HyperElasticHardening * (1 - particle.Jp));
	//}
	// else
	// if(particle.type == 3)
	// {
	// 	mu = 0;
	// 	j = particle.Jp;
	// }

	float mup = mu * e;
	float lambdap = lambda * e;
	float3x3 P = mul((2 * mup * (F - R)), transpose(F)) + lambdap * (j - 1) * j ;

	//if(particle.type == 3)
	//{
	//	float s = particle.Jp -1;
	//	P = float3x3(s,0,0,0,s,0,0,0,s) * mu * 10;
	//}
	float3x3 stress = -(_DeltaTime * volume) * mul(Dinv , P);
	float3x3 apic = stress + particle.mass * particle.C;

	// j = clamp(j, 0.6f,20.0f);
	// float3x3 FinvT = transpose(inverse(F));
	// float3x3 P = (2.0f * mup * (F - R)) + lambdap * (j - 1.0f) * j * FinvT;
	// float3x3 stress = 1.0f / j * mul(P, transpose(F));

	int3 centerCellIndex3D = ParticlePositionToCellIndex3D(particle.position);

	//for (int gx = -1; gx <= 1; ++gx) {
	//	for (int gy = -1; gy <= 1; ++gy) {
	//		for(int gz = -1; gz <=1; ++gz) {


	for (int ni = 0; ni < _CellNeighbourLength; ni++) {
		int3 cellIndex3D = centerCellIndex3D + _CellNeighbourBuffer[ni];
		uint cellIndex1D = CellIndex3DTo1D(cellIndex3D);

		//TODO(Tasuku): 後でここのInGridのifを消す
		if (InGrid(cellIndex3D))
		{
			float3 gridPositionWS = CellIndex3DToPositionWS(cellIndex3D);
			float weight = GetWeight(particle.position, _CellNeighbourBuffer[ni]);

			P2GMass p2gMass;
			p2gMass.mass = weight * particle.mass;
			p2gMass.mass_x_velocity = weight * (particle.mass * particle.velocity
				+ mul(apic, (gridPositionWS - particle.position)));
			//p2gMass.mass = 1;
			//p2gMass.mass_x_velocity = 1;

			uint laneId = particleId + ni * _NumOfParticles;

			// Used for Parallel reduction sum on mass/mass_x_velocity calculation
			// _P2GMassBuffer will be sorted by grid cell index with _GridBufferWrite
			_P2GMassBuffer[laneId] = p2gMass;

			// x = grid cell index, y = lane id of _P2GMassBuffer
			// _GridAndMassIdsBuffer will be sorted by grid cell index. 
			// Used for Parallel reduction sum on mass/mass_x_velocity calculation
			_GridAndMassIdsBuffer[laneId] = uint2(cellIndex1D, laneId);

			// For debug
			//_GridAndMassIdsBuffer[particleId + ni * _NumOfParticles] = uint2(DTid.x, ni * _NumOfParticles);
		}
	}

}


//
// Ming Gao, Xinlei Wang, Kui Wu, ..,
// GPU Optimization of Material Point Methods,
// ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia), 2018
// https://dl.acm.org/doi/10.1145/3272127.3275044
// http://pages.cs.wisc.edu/~sifakis/papers/GPU_MPM.pdf
//
[numthreads(THREAD_1D,1,1)]
void BoundaryAndInterval(
    // Unique id of entire thread
    // numthreads.x * groups.x	
	uint3 DTid : SV_DispatchThreadID,

    // Group ID
    // example:
    //   if numthreads is (4,1,1)
    //   0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3
    uint3 groupId : SV_GroupID,

    // Group index which is converted groupThreadId(3D) to 1D's number.
    // example:
    //   if numthreads is (4,1,1)
    //   0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3
	uint GI : SV_GroupIndex)
{
	const uint laneId = DTid.x;
	if (laneId-1 < 0) { return; }
	const uint cellId = _GridAndMassIdsBuffer[laneId].x;
	const uint prevCellId = _GridAndMassIdsBuffer[laneId-1].x;

	// 
	// Mark a boundary between different cells
	//
	// laneId == 0 does not have previous cell id. That's why
	// cellId != prevCellId: if cell ids are different, it is boundary point
	// GI == 0: boundary of group shared
	//
	uint boundary = (laneId == 0 || cellId != prevCellId
		|| GI == 0) ? 1 : 0;

	uint laneIdEnd = _GridIndicesBuffer[cellId].y;

	//
	// Mark region interval
	//
	// If the end of lane index is less than group shared boundary point,
	// the end is _GridIndicesBuffer[cellId].y.
	// else, the end is group shared boundary point
	//
	//uint regionInterval = laneIdEnd - 1 - laneId;
	laneIdEnd = (laneIdEnd < (groupId.x + 1) * THREAD_1D)
		? laneIdEnd
		: (groupId.x + 1) * THREAD_1D;
	uint regionInterval = laneIdEnd - 1 - laneId;

	_BoundaryAndIntervalBuffer[laneId] = uint2(boundary, regionInterval);
}


groupshared P2GMass blockP2gMass[THREAD_1D * 2];
//groupshared float blockMass[128];
//groupshared float3 blockMassXVelocity[128];

//void GatherAndWrite(
//	uint3 Gid  : SV_GroupID,
//	uint3 DTid : SV_DispatchThreadID,
//	uint3 GTid : SV_GroupThreadID,
//	uint  GI : SV_GroupIndex)
//{
[numthreads(THREAD_1D,1,1)]
void GatherAndWrite(uint3 DTid : SV_DispatchThreadID, uint GI : SV_GroupIndex)
{
	//
	// Bug memo:
	// GatherAndWriteでバグっているのは，ほぼ間違いない，
	// GridIndicesBufferあたりでバグっているか?, そもそも_BoundaryAndIntervalBufferで計算ミスしてる?
	// ま，でもbitonic sortで空の値で埋めているところは消した方がいいよなーとは思う．
	//
	// そのた
	// 表示されないパーティクルを表示させる. 
	// GatherAndWriteでの計算の不要な空パーティクルをはじく
	//
	const uint laneId = DTid.x;
	uint2 cellAndMassId = _GridAndMassIdsBuffer[laneId];
	uint cellIndex  = cellAndMassId.x;
	uint massIndex  = cellAndMassId.y;
	//uint2 startAndEndIndices = _GridIndicesBuffer[cellIndex];
	//uint startLaneId = startAndEndIndices.x;
	//uint endLaneId = startAndEndIndices.y;

	uint2 boundaryAndInterval = _BoundaryAndIntervalBuffer[laneId];
	uint boundary = boundaryAndInterval.x;
	uint regionInterval = boundaryAndInterval.y;

	// Store each particle info of 256 threads
	P2GMass p2gMass = _P2GMassBuffer[massIndex];
	blockP2gMass[GI] = p2gMass;
	GroupMemoryBarrierWithGroupSync();
	for(uint stride = 1; stride < THREAD_1D; stride <<= 1)
	{
		if (stride <= regionInterval) {
			// stride <= interval
			// only sum within the group(same grid index)
			blockP2gMass[GI].mass += blockP2gMass[GI+stride].mass;
			blockP2gMass[GI].mass_x_velocity += blockP2gMass[GI+stride].mass_x_velocity;
		}
		GroupMemoryBarrierWithGroupSync();
	}

	// Only the boundary node (Leader node) needs to write
	if (boundaryAndInterval.x)
	{
		//MpmCell cell = _GridBuffer[cellIndex];
		//cell.mass = blockP2gMass[GI].mass;
		//cell.mass_x_velocity = blockP2gMass[GI].mass_x_velocity;
		//_GridBuffer[cellIndex] = cell; //AtomicAdd is applied

		int intMass = (int) (blockP2gMass[GI].mass * FLOAT_TO_INT_DIGIT);
		//int3 intMassXVelocity = (int3) (blockP2gMass[GI].mass_x_velocity * FLOAT_TO_INT_DIGIT);
		int intMassXVelocityX = (int) (blockP2gMass[GI].mass_x_velocity.x * FLOAT_TO_INT_DIGIT);
		int intMassXVelocityY = (int) (blockP2gMass[GI].mass_x_velocity.y * FLOAT_TO_INT_DIGIT);
		int intMassXVelocityZ = (int) (blockP2gMass[GI].mass_x_velocity.z * FLOAT_TO_INT_DIGIT);
		InterlockedAdd(_GridBuffer[cellIndex].mass, intMass);
		InterlockedAdd(_GridBuffer[cellIndex].mass_x_velocity.x, intMassXVelocityX);
		InterlockedAdd(_GridBuffer[cellIndex].mass_x_velocity.y, intMassXVelocityY);
		InterlockedAdd(_GridBuffer[cellIndex].mass_x_velocity.z, intMassXVelocityZ);
	}
}
